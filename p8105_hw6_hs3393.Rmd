---
title: "p8105_hs3393_hw6"
author: "Haochen Sun"
date: "2022-11-29"
output: github_document
---

```{r pload packages, message=FALSE}
library(tidyverse)
library(ggplot2)
library(modelr)
library(leaps)

knitr::opts_chunk$set(
  fig.width = 5,
  out.width = "60%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

```

## Problem 1
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

# linear regression argument by bootstrap

boot_result <- weather_df %>% 
  bootstrap(n = 5000) %>% 
  mutate(
    model = map(strap, ~lm(tmax ~ tmin, data = .x)),
    result = map(model, broom::tidy),
    result2 = map(model, broom::glance)
  )

conf_result <- boot_result %>% 
  unnest(result) %>% 
  select(term, estimate, result2) %>% 
  unnest(result2) %>%  # two steps of unnest because some variable names overlap
  select(term, estimate, r.squared) %>% 
  pivot_wider(names_from = term, values_from = estimate) %>% 
  rename(b0 = `(Intercept)`,
         b1 = tmin) %>% 
  mutate(log_b0b1 = log(b0*b1)) %>% 
  summarise(
    rsquare_ci_lower = quantile(r.squared, 0.025),
    rsquare_ci_upper = quantile(r.squared, 0.975),
    log_b0b1_ci_lower = 
      quantile(log_b0b1, 0.025),
    log_b0b1_ci_upper = 
      quantile(log_b0b1, 0.975)
  ) %>% 
  mutate(across(where(is.numeric), round, 4)) # save 4 digits

```

Therefore, we can see that the 95% confidence interval for $\hat{r^2}$ is (`r pull(conf_result, rsquare_ci_lower)`, `r pull(conf_result, rsquare_ci_upper)` ), the 95% confidence interval for $log(\hat{\beta_0}*\hat{\beta_1})$ is (`r pull(conf_result, log_b0b1_ci_lower)`, `r pull(conf_result, log_b0b1_ci_upper)`).

## problem 2
```{r message=FALSE}
data <- read_csv("data/homicide-data.csv") %>% 
  janitor::clean_names() %>%
  unite("city_state", c(city, state), sep = ", ", remove = F) %>%
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) %>% 
  mutate(solve = if_else(
    disposition != "Closed by arrest",
    true = 0, false = 1
  )) %>% # 0 means the case is unsolved 
    filter(victim_race %in% c("Black", "White")) %>% 
  mutate(victim_age =  as.numeric(victim_age))

balt_model <- baltimore <- data %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(solve ~ victim_age + victim_sex + victim_race, data = .,family = binomial())

balt_result <- balt_model %>%
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>% 
  mutate(lower = exp(confint(balt_model)[,1]),
         upper = exp(confint(balt_model)[,2])
  ) %>% 
  select(term, OR, lower, upper) %>%
  filter(term == "victim_sexMale") %>% mutate(across(where(is.numeric), round, 4))
```

The estimated odds ratio for solving homicides comparing male victims to victims is `r balt_result[,"OR"]`, the 95% confidence interval for it is (`r balt_result[,"lower"]`, `r balt_result[,"upper"]`). 

```{r, warning=FALSE, message=FALSE}
# Glm for all cities

all_model <- data %>% 
  nest(data = -city_state) %>% 
  mutate(model = map(data, ~glm(solve ~ victim_age + victim_sex + victim_race, data = .x,family = binomial()))
         ) %>%
  mutate(
    results = map(model, broom::tidy)
  )

all_result <- all_model %>% 
  select(city_state, model, results) %>% mutate(
    lower = map(.x = model, ~exp(confint(object = .x)[,1])),
    upper = map(.x = model, ~exp(confint(object = .x)[,2])) 
    )%>% 
  select(-model) %>% 
  unnest(results, lower, upper) %>% 
  mutate(OR = exp(estimate)) %>% 
  select(city_state, term, OR, lower, upper) %>% 
  filter(term == "victim_sexMale")

all_result %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR, color = city_state)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) + 
  coord_flip() +
  theme(legend.position="none")+
  labs(x = "Adjusted odds ratio", 
       y = "City, State",
       title = "Homicides solving odds ratio male & female" )
```

Homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female in many cities, eg. New York, Chicago and Baltimore. In no city homicides in which victim is female are significantly less likely to be resolved as no lower bound of the odds ratio is less than 1.

## Problem 3
```{r}
data <- read_csv("data/birthweight.csv") %>% 
  mutate_at(c("babysex", "frace", "malform", "mrace"), as.factor)

which(is.na(data) == T)
```

The dataset have no NA.

### Model building

We will start model building by stepwise variable selection by adjusted R squared.

```{r determine variable number}
mat <- as.matrix(data)

models <- regsubsets(bwt ~ ., data = data, nvmax = 19)

model_summary <- summary(models)

plot(model_summary$adjr2, xlab="No of parameters", ylab="Adj R2")

diff(model_summary$adjr2)

coef(models, id = 4)
```

My modeling procedures are: first use `regsubsets` function in `leap` package, plot adjusted R squared against variable numbers. Then, find the optimal variable numbers, use `coef` function to select the best 4 variables to fit in the linear model. 

Use adjusted R squared as the criteria, we can see that after the variable number greater than 4, the contribution of adding a new variable will help adjusted R squared increase less than 0.005 (subjectively selected), which means adding them will have trivial contribution to the model, so we use 4 as the cutoff point. During that process as mrace2 is indicated as a important variable, it would be no harm to put mrace all in the model.

```{r selected variable model}
select_model <- lm(bwt ~ bhead + blength + delwt + mrace, data = data)

data %>%
  add_predictions(select_model) %>% 
  add_residuals(select_model) %>% 
  ggplot(aes(x = pred, y = resid))+
  geom_point()+
  labs(
    x = "Predicted weight (g)",
    y = "Residuals",
    title = "Residual vs Predicted Value"
  )

cv_df <- 
  crossv_mc(data, 100) %>%
  mutate(
    select_mod = map(
      train, ~lm(bwt ~ bhead + blength + delwt + mrace, data = .x)
    ),
    main_2_mod = map(
      train, ~lm(bwt ~ blength + gaweeks, data = .x)
    ),
    inter_3_mod = map(
      train, ~lm(bwt ~ bhead + blength + babysex + bhead:blength + bhead:babysex + blength:babysex + bhead:blength:babysex, data = .x))
  ) %>% 
  mutate(
    rmse_select = map2_dbl(select_mod, test, ~rmse(model = .x, data = .y)),
    rmse_main2 = map2_dbl(main_2_mod, test, ~rmse(model = .x, data = .y)),
    rmse_inter3 = map2_dbl(inter_3_mod, test, ~rmse(model = .x, data = .y))
  )

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin(aes(color = model)) +
  labs(x = "Models",
       y = "RMSE"
         )

```

The selected model (including 4 variables) have a generally lower RMSE, having better performance in predicting, while the model have 2 main effects (length at birth and gestational age) have higher RMSE and preform worst in prediction among the three models.